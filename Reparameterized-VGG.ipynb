{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29f510b",
   "metadata": {},
   "source": [
    "## Implementation for the paper RepVGG: Making VGG-style ConvNets Great Again\n",
    "I only implemented the inference-time model for the idea of RepVGG. I wrote this code just for the beauty of some deep-learning ideas and the fun of playing Pytorch. The idea of RepVGG is presented in the following screenshot, which is taken from the original paper. Specifically, the goal is to only use 3x3 convolution kernel to represent [3x3 + 1x1 + identity], which will save computation costs. Because it puts three operations into one tensor(if we omit biases).\n",
    "![RepVGG](./RepVGG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7efb3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Generate the data of one image\n",
    "data = torch.randn(1,2,256,256)\n",
    "in_channels = 2\n",
    "ou_channels = 2\n",
    "kernel_size = 3\n",
    "\n",
    "# original method\n",
    "conv_layer = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=\"same\")\n",
    "pointwise_layer = nn.Conv2d(in_channels, ou_channels, 1) # kernel_size=1\n",
    "out1 = conv_layer(data) + pointwise_layer(data) + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "845cf861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only use 3x3 convolution kernel\n",
    "## transform the 1*1 kernel of the pointwise layer into 3*3 kernel\n",
    "pointwise_weight = F.pad(pointwise_layer.weight.data, pad=(1,1,1,1)) # 2*2*1*1 -> 2*2*3*3\n",
    "conv2d_pointwise = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=\"same\")\n",
    "conv2d_pointwise.weight = nn.Parameter(pointwise_weight)\n",
    "conv2d_pointwise.bias = pointwise_layer.bias\n",
    "\n",
    "## perform the identity operation via convolution of 3*3 kernels\n",
    "ones = torch.unsqueeze(F.pad(torch.ones(1,1), pad=(1,1,1,1)), dim=0)  \n",
    "zeros = torch.unsqueeze(torch.zeros(kernel_size,kernel_size), dim=0)\n",
    "out_channel1_identity =torch.unsqueeze(torch.cat([ones,zeros], dim=0), dim=0)\n",
    "out_channel2_identity = torch.unsqueeze(torch.cat([zeros,ones], dim=0), dim=0)\n",
    "identity_layer_weight = torch.cat([out_channel1_identity,out_channel2_identity], dim=0)\n",
    "identity_layer_bias = torch.zeros(ou_channels)\n",
    "\n",
    "conv2d_identity_layer = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=\"same\")\n",
    "conv2d_identity_layer.weight = nn.Parameter(identity_layer_weight)\n",
    "conv2d_identity_layer.bias = nn.Parameter(identity_layer_bias)\n",
    "\n",
    "out2 = conv_layer(data) + conv2d_pointwise(data) + conv2d_identity_layer(data)\n",
    "torch.allclose(out1, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51790ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fusion\n",
    "fusion_layer = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=\"same\")\n",
    "fusion_layer.weight = nn.Parameter(conv_layer.weight.data + conv2d_pointwise.weight.data + conv2d_identity_layer.weight.data)\n",
    "fusion_layer.bias = nn.Parameter(conv_layer.bias.data + conv2d_pointwise.bias.data + conv2d_identity_layer.bias.data)\n",
    "out3 = fusion_layer(data)\n",
    "torch.allclose(out2,out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "85563b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out1.detach()-out2.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ddd1248c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5367e-07)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out1.detach()-out3.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e0dfc908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.006000041961669922\n",
      "Time: 0.0019991397857666016\n"
     ]
    }
   ],
   "source": [
    "# compare their inference times\n",
    "import time\n",
    "t1 = time.time()\n",
    "out1 = conv_layer(data) + pointwise_layer(data) + data # original method\n",
    "t2 = time.time()\n",
    "print(f\"Time: {t2-t1}\")\n",
    "\n",
    "t1 = time.time()\n",
    "out3 = fusion_layer(data) # fusion method\n",
    "t2 = time.time()\n",
    "print(f\"Time: {t2-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27eb51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
